---
description: About this website
toc_min_heading_level: 2
toc_max_heading_level: 5
hide_table_of_contents: true
---

# About

I started this website because I needed some data to test vector embeddings[^1] to use alongside of large language models[^2] for natural language processing[^3]. If this reads like gibberish, just know that these are used in machine learning[^4] and artificial intelligence[^5].

## Who

My name is David Windham and I live in Greenwood, South Carolina.  
I work as a software and web developer.  
My website is @ [davidwindham.com](https://davidwindham.com)

## What

In order to train and have complete control over AI agents, I need to use just a very small amount of relatively anonymous and public data. I've decided to start with that I know. I've previously built a couple AI agents using some common tools but I want this one to rely on only one dependency and be interoperable with an assortment of models.

## Where

I have lived in South Carolina most of my life. I've lived in Greenwood for 14 years. Greenwood is a relatively small town in Greenwood County but it's large enough to serve the adjoining six counties.

## When

I started this in late 2024 and I've learned that refining data takes a lot of time. I have no timeline and I've found that organizing the information to be an organic process. I'm keeping a dated log of the build @ [/dev](/dev)

## Why

I'm building this mostly as a learning tool. I also tend to think that the web is a lot like a community garden. Part of the modern web tends to be gated or walled. The information that lives behind those walls isn't always accessible or easy to navigate. I'd like the end result to be an easy to use reference for my little spot on the internet and our planet.

## How

This website is powered by static [HTML](https://en.wikipedia.org/wiki/HTML) generated using [Docusaurus](https://docusaurus.io). It allows me to keep all of the documents in a simple [Markdown](https://en.wikipedia.org/wiki/Markdown) so that it's easier to automate the parsing into a vector database. I've automated the build build, making the code publicly available, and documenting the proccess @ [/dev](/dev) 

<div>&nbsp;</div>

Please feel free to [contact me](https://davidawindham.com/contact) if you have any questions.

<div>&nbsp;</div>

---

[^1]: [Vector database](https://en.wikipedia.org/wiki/Vector_database)
[^2]: [Large language model](https://en.wikipedia.org/wiki/Large_language_model)
[^3]: [Natural language processing](https://en.wikipedia.org/wiki/Natural_language_processing)
[^4]: [Machine learning](https://en.wikipedia.org/wiki/Machine_learning)
[^5]: [Artificial intelligence](https://en.wikipedia.org/wiki/Artificial_intelligence)
